{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5de4e542fd444a0a790fc2299462098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f84ab62858d461380ec7a55280c1133",
              "IPY_MODEL_e23a19d2ecb44094a19b4a1e26a61814",
              "IPY_MODEL_dbb2bf82869e4e0aa23287e9e7b24c86"
            ],
            "layout": "IPY_MODEL_dea837a8a6c74f45a0952646511c00c9"
          }
        },
        "7f84ab62858d461380ec7a55280c1133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38ffda95944d43c3b86ed01de7412e8c",
            "placeholder": "​",
            "style": "IPY_MODEL_cf919c17634d46c683a33ba5a8755f0b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e23a19d2ecb44094a19b4a1e26a61814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f94b11a67ef245738159df7910232462",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0050c45f4074e7bb874110a5e01cac7",
            "value": 33
          }
        },
        "dbb2bf82869e4e0aa23287e9e7b24c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d8f0b55fc2b497096d95aa5d78365fd",
            "placeholder": "​",
            "style": "IPY_MODEL_663b9610433f45a18849d0a8b8e56eb2",
            "value": " 33/33 [02:02&lt;00:00,  3.68s/it]"
          }
        },
        "dea837a8a6c74f45a0952646511c00c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ffda95944d43c3b86ed01de7412e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf919c17634d46c683a33ba5a8755f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f94b11a67ef245738159df7910232462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0050c45f4074e7bb874110a5e01cac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d8f0b55fc2b497096d95aa5d78365fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "663b9610433f45a18849d0a8b8e56eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etH6D9NeUse9",
        "outputId": "00bd49bc-0526-483c-f1de-fc0db08d3eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet transformers==4.34.1 accelerate==0.24.0 sentencepiece==0.1.99 optimum==1.13.2 peft==0.5.0 bitsandbytes==0.41.2.post2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from tqdm.auto import tqdm , trange\n",
        "assert torch.cuda.is_available(),\"you need cuda for this\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name,device_map = device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name , device_map = 'auto' , low_cpu_mem_usage = True , offload_state_dict = True ,\n",
        "    load_in_4bit = True , torch_dtype = torch.float32 , #weights are 4 bit ,activation and layernorm are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "d5de4e542fd444a0a790fc2299462098",
            "7f84ab62858d461380ec7a55280c1133",
            "e23a19d2ecb44094a19b4a1e26a61814",
            "dbb2bf82869e4e0aa23287e9e7b24c86",
            "dea837a8a6c74f45a0952646511c00c9",
            "38ffda95944d43c3b86ed01de7412e8c",
            "cf919c17634d46c683a33ba5a8755f0b",
            "f94b11a67ef245738159df7910232462",
            "a0050c45f4074e7bb874110a5e01cac7",
            "5d8f0b55fc2b497096d95aa5d78365fd",
            "663b9610433f45a18849d0a8b8e56eb2"
          ]
        },
        "id": "L08WtqJ5WlXD",
        "outputId": "e3a7f61c-6d2e-49e9-923b-983b7fca656e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5de4e542fd444a0a790fc2299462098"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=map_location)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROMPT TUNE THE STORY OF A FOX **"
      ],
      "metadata": {
        "id": "eaIvD_ccbwhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "batch\n",
        "for i in range(10):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcCf-XDKgefd",
        "outputId": "8151c235-4b67-4a13-fb7b-b5bd3b1a97a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output: <s>A quick brown fox jumps over the lazy dog.\n",
            "A quick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "outputs = model(**batch)\n",
        "#print(outputs)\n",
        "next_word_logits = outputs.logits[:, :-1]\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "print(\"Loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha3L0E03iAt_",
        "outputId": "1bc1e563-35ef-495e-95cf-cb380b55d619"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(3.0725, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the model cant be trained as it has large amount of parameters"
      ],
      "metadata": {
        "id": "qoLCFQdkl8Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import peft\n",
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n",
        "\n",
        "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n",
        "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n",
        "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MJsKm3dAdnG-",
        "outputId": "8e5fa3a1-a19c-4fca-89c4-8afe7c944bdd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 65536\n",
            "Total parameters (excluding quantization): 3500478464\n",
            "trainable params: 65,536 || all params: 6,738,481,152 || trainable%: 0.0009725633792200893\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step -2 is to train the trainable parameters in peft"
      ],
      "metadata": {
        "id": "nxsTpVCwsmiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False)\n",
        "batch = batch.to(device)\n",
        "outputs = model(**batch)\n",
        "print(outputs[0].shape)\n",
        "print(batch['input_ids'].shape)\n",
        "next_word_logits = outputs.logits[:, 16 : -1, :]\n",
        "print(next_word_logits.shape)\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "print(true_next_tokens.shape)\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "print(\"Loss:\", loss)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(60):\n",
        "  opt.zero_grad()\n",
        "  outputs = model(**batch)\n",
        "  next_word_logits = outputs.logits[:, 16 : -1, :]\n",
        "  true_next_tokens = batch['input_ids'][:, 1:]\n",
        "  loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "  print(f\"Epoch {epoch + 1}: Loss {loss.item()}\")\n",
        "  if loss.item() < 0.1:\n",
        "    print(\"Looks good!\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlhWFF7X-GyF",
        "outputId": "04b02c03-1ec9-490f-c6ad-f08aadfda732"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 39, 32000])\n",
            "torch.Size([1, 23])\n",
            "torch.Size([1, 22, 32000])\n",
            "torch.Size([1, 22])\n",
            "Loss: tensor(5.2252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch 1: Loss 5.225242614746094\n",
            "Epoch 2: Loss 5.164522171020508\n",
            "Epoch 3: Loss 5.10544490814209\n",
            "Epoch 4: Loss 5.047178745269775\n",
            "Epoch 5: Loss 4.989747524261475\n",
            "Epoch 6: Loss 4.932901859283447\n",
            "Epoch 7: Loss 4.876589775085449\n",
            "Epoch 8: Loss 4.820857524871826\n",
            "Epoch 9: Loss 4.765735149383545\n",
            "Epoch 10: Loss 4.711192607879639\n",
            "Epoch 11: Loss 4.657163619995117\n",
            "Epoch 12: Loss 4.603607177734375\n",
            "Epoch 13: Loss 4.550509452819824\n",
            "Epoch 14: Loss 4.497833251953125\n",
            "Epoch 15: Loss 4.445532321929932\n",
            "Epoch 16: Loss 4.393546104431152\n",
            "Epoch 17: Loss 4.341867446899414\n",
            "Epoch 18: Loss 4.290485858917236\n",
            "Epoch 19: Loss 4.239383220672607\n",
            "Epoch 20: Loss 4.188554763793945\n",
            "Epoch 21: Loss 4.13797664642334\n",
            "Epoch 22: Loss 4.087636947631836\n",
            "Epoch 23: Loss 4.037542819976807\n",
            "Epoch 24: Loss 3.9876744747161865\n",
            "Epoch 25: Loss 3.938027858734131\n",
            "Epoch 26: Loss 3.888590097427368\n",
            "Epoch 27: Loss 3.8393523693084717\n",
            "Epoch 28: Loss 3.7903082370758057\n",
            "Epoch 29: Loss 3.741438388824463\n",
            "Epoch 30: Loss 3.692732810974121\n",
            "Epoch 31: Loss 3.644176483154297\n",
            "Epoch 32: Loss 3.5957491397857666\n",
            "Epoch 33: Loss 3.547447443008423\n",
            "Epoch 34: Loss 3.499262809753418\n",
            "Epoch 35: Loss 3.4511711597442627\n",
            "Epoch 36: Loss 3.4031777381896973\n",
            "Epoch 37: Loss 3.3552727699279785\n",
            "Epoch 38: Loss 3.307457208633423\n",
            "Epoch 39: Loss 3.259720802307129\n",
            "Epoch 40: Loss 3.2120604515075684\n",
            "Epoch 41: Loss 3.164475440979004\n",
            "Epoch 42: Loss 3.116957187652588\n",
            "Epoch 43: Loss 3.069500207901001\n",
            "Epoch 44: Loss 3.022101402282715\n",
            "Epoch 45: Loss 2.974748134613037\n",
            "Epoch 46: Loss 2.9274444580078125\n",
            "Epoch 47: Loss 2.880180835723877\n",
            "Epoch 48: Loss 2.832962989807129\n",
            "Epoch 49: Loss 2.785778284072876\n",
            "Epoch 50: Loss 2.738647699356079\n",
            "Epoch 51: Loss 2.691560745239258\n",
            "Epoch 52: Loss 2.644533395767212\n",
            "Epoch 53: Loss 2.597581148147583\n",
            "Epoch 54: Loss 2.550713300704956\n",
            "Epoch 55: Loss 2.503953218460083\n",
            "Epoch 56: Loss 2.457308769226074\n",
            "Epoch 57: Loss 2.4108080863952637\n",
            "Epoch 58: Loss 2.364478826522827\n",
            "Epoch 59: Loss 2.318331003189087\n",
            "Epoch 60: Loss 2.272402763366699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWldlKFpT9Nh",
        "outputId": "a2f398bb-2114-4649-f7ea-204d0b45e0e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output: <s>A quick brown fox frown frown frown frown frown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYTQaADaUlYl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
